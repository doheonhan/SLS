{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79042d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9bd17",
   "metadata": {},
   "source": [
    "# 1. MNIST_784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d14b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets.mnist import load_data\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be96cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "class_names = ['0', '1', '2', '3', '4', '5', '6', '7','8','9']\n",
    "sample_size = 9\n",
    "random_idx = np.random.randint(10000, size=sample_size)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "for i, idx in enumerate(random_idx):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(X_test[idx], cmap='gray')\n",
    "    plt.xlabel(class_names[y_test[idx]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.repeat(X_test[..., np.newaxis], 3, -1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604b55d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class_names = ['0', '1', '2', '3', '4', '5', '6', '7','8','9']\n",
    "y_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    if y_test[i] in y_list:\n",
    "        continue\n",
    "    y_list.append(y_test[i])    \n",
    "    some_digit = X_test[i]\n",
    "    # some_digit_image = some_digit.reshape(28, 28)\n",
    "\n",
    "    plt.imshow(some_digit, cmap = \"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(y_test[i], class_names[y_test[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e76187",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ac960",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_7 = []\n",
    "idx_9 = []\n",
    "for i in range(len(y_test)):\n",
    "    if list(y_test)[i] == 7:\n",
    "        idx_7.append(i)\n",
    "    if list(y_test)[i] == 9:\n",
    "        idx_9.append(i)\n",
    "print(len(idx_7), len(idx_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344dce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx_7[:50] + idx_9[:50]\n",
    "idx.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in idx:\n",
    "    X.append(X_test[i])\n",
    "    y.append(y_test[i])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ad3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hard = [0.00 if x==7 else x for x in y]\n",
    "y_hard = [1.00 if x==9 else x for x in y_hard]\n",
    "pd.Series(y_hard).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc63b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "epochs = 10\n",
    "batch = 32\n",
    "decay=0.0001\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde2d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Dropout\n",
    "\n",
    "# transformer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "num_classes = 1\n",
    "input_shape = (28, 28, 3)\n",
    "\n",
    "image_size = 54  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "    \n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes, activation='sigmoid')(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce1007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hard = pd.DataFrame(y_hard)\n",
    "y_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c331ee",
   "metadata": {},
   "source": [
    "# 1-0. Generating Prob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9322fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = create_vit_classifier()   \n",
    "gen_model.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.0002), metrics=['accuracy'])\n",
    "history = gen_model.fit(X, y_hard, validation_split=0.2, epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "result = gen_model.predict(X, verbose=0)\n",
    "prob_label = list(result.reshape(len(X),))\n",
    "prob_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6aef0",
   "metadata": {},
   "source": [
    "# 1-1. Focal(Hard) and SLS(Hard/diverse alphas)_option#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d78a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10):    # 10 times repeat    \n",
    "    res = pd.DataFrame({'Focal':[0, 0, 0, 0, 0]}, index = ['Acc','Pre','Rec','F1','R-AUC']) \n",
    "    # Focal\n",
    "    print('#'*50,'Focal','#'*50)\n",
    "    list_acc = []\n",
    "    list_pre = []\n",
    "    list_rec = []\n",
    "    list_f1 = []\n",
    "    list_rauc = []   \n",
    "    focal_model = create_vit_classifier()   \n",
    "\n",
    "    n_iter = 0\n",
    "    for train_index, test_index in skf.split(X, y_hard):  # straticiation by y_hard(binary label)\n",
    "        n_iter += 1\n",
    "        X_train = X[train_index]\n",
    "        y_train= y_hard.iloc[train_index]\n",
    "        if n_iter == 1:\n",
    "            print(y_train.value_counts())\n",
    "        X_test = X[test_index]\n",
    "        y_test= y_hard.iloc[test_index]\n",
    "#         print('#'*10,'{0}th CV'.format(n_iter),'#'*10)\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        y_train = y_train.astype(float)\n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "        y_test = y_test.astype(float)\n",
    "\n",
    "        focal_model.compile(loss='BinaryFocalCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.0002), metrics=['accuracy'])\n",
    "        history = focal_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        predicted = np.round(focal_model.predict(X_test, verbose=0))\n",
    "        list_acc.append(metrics.accuracy_score(y_test, predicted))\n",
    "        list_pre.append(metrics.precision_score(y_test, predicted))\n",
    "        list_rec.append(metrics.recall_score(y_test, predicted))\n",
    "        list_f1.append(metrics.f1_score(y_test, predicted))\n",
    "        list_rauc.append(metrics.roc_auc_score(y_test, predicted))\n",
    "    res['Focal'] = [np.mean(list_acc), np.mean(list_pre), np.mean(list_rec), np.mean(list_f1), np.mean(list_rauc)]\n",
    "    print([np.mean(list_acc), np.mean(list_pre), np.mean(list_rec), np.mean(list_f1), np.mean(list_rauc)])\n",
    "    \n",
    "    B = [0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10]  # SLS with LS\n",
    "    c = 0.1  # criterion decides easy/hard\n",
    "    for b in B:\n",
    "        print('#'*50,'SLS',b,'#'*50)\n",
    "        y_005 = []\n",
    "        for i in range(len(y_hard)):\n",
    "            if list(y_hard[0])[i] == 0:\n",
    "                if prob_label[i] <= c:\n",
    "                    y_005.append(b)  # easy sample\n",
    "                else:\n",
    "                    y_005.append(0) # (or 0-b) hard sample\n",
    "            if list(y_hard[0])[i] == 1:\n",
    "                if prob_label[i] >= 1-c:\n",
    "                    y_005.append(1-b)  # easy sample\n",
    "                else:\n",
    "                    y_005.append(1) # (or 1+b) hard sample\n",
    "        y_005 = pd.DataFrame(y_005)     \n",
    "\n",
    "        bce005_acc = []\n",
    "        bce005_pre = []\n",
    "        bce005_rec = []\n",
    "        bce005_f1 = []\n",
    "        bce005_rocauc = []\n",
    "        model_005 = create_vit_classifier()\n",
    "#             early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        n_iter = 0\n",
    "        for train_index, test_index in skf.split(X, y_hard):  # straticiation by y_hard(binary label)\n",
    "            n_iter += 1\n",
    "            X_train = X[train_index]\n",
    "            y_005_train= y_005.iloc[train_index]\n",
    "            if n_iter == 1:\n",
    "                print(y_005_train.value_counts())\n",
    "            X_test = X[test_index]\n",
    "            y_test= y_hard.iloc[test_index]  # test with real(actual) label y\n",
    "#                 print('#'*10,'{0}th CV'.format(n_iter),'#'*10)\n",
    "            X_train = np.array(X_train)\n",
    "            y_005_train = np.array(y_005_train)\n",
    "            y_005_train = y_005_train.astype(float)\n",
    "            X_test = np.array(X_test)\n",
    "            y_test = np.array(y_test)\n",
    "            y_test = y_test.astype(float)\n",
    "\n",
    "            # MLP_BCE(y_005)\n",
    "            model_005.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.0002), metrics=['accuracy'])\n",
    "            history = model_005.fit(X_train, y_005_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#             plt.plot(history.history['loss'], label='loss')\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "            predicted = np.round(model_005.predict(X_test, verbose=0))\n",
    "            bce005_acc.append(metrics.accuracy_score(y_test, predicted))\n",
    "            bce005_pre.append(metrics.precision_score(y_test, predicted))\n",
    "            bce005_rec.append(metrics.recall_score(y_test, predicted))\n",
    "            bce005_f1.append(metrics.f1_score(y_test, predicted))\n",
    "            bce005_rocauc.append(metrics.roc_auc_score(y_test, predicted))\n",
    "        res['SLS({})'.format(b)] = [np.mean(bce005_acc), np.mean(bce005_pre), np.mean(bce005_rec), np.mean(bce005_f1), np.mean(bce005_rocauc)]\n",
    "        print([np.mean(bce005_acc), np.mean(bce005_pre), np.mean(bce005_rec), np.mean(bce005_f1), np.mean(bce005_rocauc)])        \n",
    "    res.to_csv(\"ViT_MNIST_5CV(SLS_opt#1_c0.1).csv\", mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c6997",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# smote\n",
    "res = pd.read_csv(\"ViT_MNIST_5CV(SLS_opt#1_c0.1).csv\")\n",
    "res = res.dropna(axis=0)\n",
    "res = res.rename(columns={'SLS(0.0)':'Hard'})\n",
    "df_acc = res[res.iloc[:,0] == 'Acc']\n",
    "df_acc = df_acc.reset_index(drop=True)\n",
    "df_acc = df_acc.iloc[:,1:].astype(float)\n",
    "col_name = df_acc.columns\n",
    "ave = []\n",
    "std = []\n",
    "for i in range(12):\n",
    "    ave.append(np.mean(list(df_acc.iloc[:,i])))\n",
    "    std.append(np.std(list(df_acc.iloc[:,i])))\n",
    "final = pd.DataFrame(ave, index=col_name, columns=[\"mean\"])\n",
    "final['std'] = std\n",
    "final.sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f040b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a307a27",
   "metadata": {},
   "source": [
    "# 2. Fashion_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets.fashion_mnist import load_data\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8496b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker','Bag','Ankle boot']\n",
    "sample_size = 9\n",
    "random_idx = np.random.randint(10000, size=sample_size)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "for i, idx in enumerate(random_idx):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(X_test[idx], cmap='gray')\n",
    "    plt.xlabel(class_names[y_test[idx]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.repeat(X_test[..., np.newaxis], 3, -1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32777f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker','Bag','Ankle boot']\n",
    "y_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    if y_test[i] in y_list:\n",
    "        continue\n",
    "    y_list.append(y_test[i])    \n",
    "    some_digit = X_test[i]\n",
    "    # some_digit_image = some_digit.reshape(28, 28)\n",
    "\n",
    "    plt.imshow(some_digit, cmap = \"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(y_test[i], class_names[y_test[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03415a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking only label (4, 6)->0 & 2->1\n",
    "idx_2 = []\n",
    "idx_4 = []\n",
    "# idx_6 = []\n",
    "for i in range(len(y_test)):\n",
    "    if list(y_test)[i] == 2:\n",
    "        idx_2.append(i)\n",
    "    if list(y_test)[i] == 4:\n",
    "        idx_4.append(i)\n",
    "#     if list(y_test)[i] == 6:\n",
    "#         idx_6.append(i)\n",
    "print(len(idx_2), len(idx_4)) #, len(idx_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00435758",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx_2[:50] + idx_4[:50] # + idx_6[:110]\n",
    "idx.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c431b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in idx:\n",
    "    X.append(X_test[i])\n",
    "    y.append(y_test[i])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195459d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hard = [0.00 if x==2 else x for x in y]\n",
    "y_hard = [1.00 if x==4 else x for x in y_hard]\n",
    "pd.Series(y_hard).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "epochs = 10\n",
    "batch = 32\n",
    "decay=0.0001\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Dropout\n",
    "\n",
    "# transformer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "num_classes = 1\n",
    "input_shape = (28, 28, 3)\n",
    "\n",
    "image_size = 54  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "    \n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes, activation='sigmoid')(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hard = pd.DataFrame(y_hard)\n",
    "y_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1423fb5d",
   "metadata": {},
   "source": [
    "# 2-0. Generating Prob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840802a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = create_vit_classifier()   \n",
    "gen_model.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.0001), metrics=['accuracy'])\n",
    "history = gen_model.fit(X, y_hard, validation_split=0.2, epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "result = gen_model.predict(X, verbose=0)\n",
    "prob_label = list(result.reshape(len(X),))\n",
    "prob_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb10c9b",
   "metadata": {},
   "source": [
    "# 2-1. Focal(Hard) and SLS(Hard/diverse alphas)_option#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10):    # 10 times repeat    \n",
    "    res = pd.DataFrame({'Focal':[0, 0, 0, 0, 0]}, index = ['Acc','Pre','Rec','F1','R-AUC']) \n",
    "    # Focal\n",
    "    print('#'*50,'Focal','#'*50)\n",
    "    list_acc = []\n",
    "    list_pre = []\n",
    "    list_rec = []\n",
    "    list_f1 = []\n",
    "    list_rauc = []   \n",
    "    focal_model = create_vit_classifier()   \n",
    "\n",
    "    n_iter = 0\n",
    "    for train_index, test_index in skf.split(X, y_hard):  # straticiation by y_hard(binary label)\n",
    "        n_iter += 1\n",
    "        X_train = X[train_index]\n",
    "        y_train= y_hard.iloc[train_index]\n",
    "        if n_iter == 1:\n",
    "            print(y_train.value_counts())\n",
    "        X_test = X[test_index]\n",
    "        y_test= y_hard.iloc[test_index]\n",
    "#         print('#'*10,'{0}th CV'.format(n_iter),'#'*10)\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        y_train = y_train.astype(float)\n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "        y_test = y_test.astype(float)\n",
    "\n",
    "        focal_model.compile(loss='BinaryFocalCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.0001), metrics=['accuracy'])\n",
    "        history = focal_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        predicted = np.round(focal_model.predict(X_test, verbose=0))\n",
    "        list_acc.append(metrics.accuracy_score(y_test, predicted))\n",
    "        list_pre.append(metrics.precision_score(y_test, predicted))\n",
    "        list_rec.append(metrics.recall_score(y_test, predicted))\n",
    "        list_f1.append(metrics.f1_score(y_test, predicted))\n",
    "        list_rauc.append(metrics.roc_auc_score(y_test, predicted))\n",
    "    res['Focal'] = [np.mean(list_acc), np.mean(list_pre), np.mean(list_rec), np.mean(list_f1), np.mean(list_rauc)]\n",
    "    print([np.mean(list_acc), np.mean(list_pre), np.mean(list_rec), np.mean(list_f1), np.mean(list_rauc)])\n",
    "    \n",
    "    B = [0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10]  # SLS with LS\n",
    "    c = 0.1  # criterion decides easy/hard\n",
    "    for b in B:\n",
    "        print('#'*50,'SLS',b,'#'*50)\n",
    "        y_005 = []\n",
    "        for i in range(len(y_hard)):\n",
    "            if list(y_hard[0])[i] == 0:\n",
    "                if prob_label[i] <= c:\n",
    "                    y_005.append(b)  # easy sample\n",
    "                else:\n",
    "                    y_005.append(0) # (or 0-b) hard sample\n",
    "            if list(y_hard[0])[i] == 1:\n",
    "                if prob_label[i] >= 1-c:\n",
    "                    y_005.append(1-b)  # easy sample\n",
    "                else:\n",
    "                    y_005.append(1) # (or 1+b) hard sample\n",
    "        y_005 = pd.DataFrame(y_005)     \n",
    "\n",
    "        bce005_acc = []\n",
    "        bce005_pre = []\n",
    "        bce005_rec = []\n",
    "        bce005_f1 = []\n",
    "        bce005_rocauc = []\n",
    "        model_005 = create_vit_classifier()\n",
    "#             early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        n_iter = 0\n",
    "        for train_index, test_index in skf.split(X, y_hard):  # straticiation by y_hard(binary label)\n",
    "            n_iter += 1\n",
    "            X_train = X[train_index]\n",
    "            y_005_train= y_005.iloc[train_index]\n",
    "            if n_iter == 1:\n",
    "                print(y_005_train.value_counts())\n",
    "            X_test = X[test_index]\n",
    "            y_test= y_hard.iloc[test_index]  # test with real(actual) label y\n",
    "#                 print('#'*10,'{0}th CV'.format(n_iter),'#'*10)\n",
    "            X_train = np.array(X_train)\n",
    "            y_005_train = np.array(y_005_train)\n",
    "            y_005_train = y_005_train.astype(float)\n",
    "            X_test = np.array(X_test)\n",
    "            y_test = np.array(y_test)\n",
    "            y_test = y_test.astype(float)\n",
    "\n",
    "            # MLP_BCE(y_005)\n",
    "            model_005.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.0001), metrics=['accuracy'])\n",
    "            history = model_005.fit(X_train, y_005_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#             plt.plot(history.history['loss'], label='loss')\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "            predicted = np.round(model_005.predict(X_test, verbose=0))\n",
    "            bce005_acc.append(metrics.accuracy_score(y_test, predicted))\n",
    "            bce005_pre.append(metrics.precision_score(y_test, predicted))\n",
    "            bce005_rec.append(metrics.recall_score(y_test, predicted))\n",
    "            bce005_f1.append(metrics.f1_score(y_test, predicted))\n",
    "            bce005_rocauc.append(metrics.roc_auc_score(y_test, predicted))\n",
    "        res['SLS({})'.format(b)] = [np.mean(bce005_acc), np.mean(bce005_pre), np.mean(bce005_rec), np.mean(bce005_f1), np.mean(bce005_rocauc)]\n",
    "        print([np.mean(bce005_acc), np.mean(bce005_pre), np.mean(bce005_rec), np.mean(bce005_f1), np.mean(bce005_rocauc)])        \n",
    "    res.to_csv(\"ViT_F_MNIST_5CV(SLS_opt#1_c0.1).csv\", mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d7fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote\n",
    "res = pd.read_csv(\"ViT_F_MNIST_5CV(SLS_opt#1_c0.1).csv\")\n",
    "res = res.dropna(axis=0)\n",
    "res = res.rename(columns={'SLS(0.0)':'Hard'})\n",
    "df_acc = res[res.iloc[:,0] == 'Acc']\n",
    "df_acc = df_acc.reset_index(drop=True)\n",
    "df_acc = df_acc.iloc[:,1:].astype(float)\n",
    "col_name = df_acc.columns\n",
    "ave = []\n",
    "std = []\n",
    "for i in range(12):\n",
    "    ave.append(np.mean(list(df_acc.iloc[:,i])))\n",
    "    std.append(np.std(list(df_acc.iloc[:,i])))\n",
    "final = pd.DataFrame(ave, index=col_name, columns=[\"mean\"])\n",
    "final['std'] = std\n",
    "final.sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbba0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24faeedc",
   "metadata": {},
   "source": [
    "# 3. CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785216d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse','ship','truck']\n",
    "y_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    if y_test[i] in y_list:\n",
    "        continue\n",
    "    y_list.append(y_test[i])    \n",
    "    some_digit = X_test[i]\n",
    "    # some_digit_image = some_digit.reshape(28, 28)\n",
    "\n",
    "    plt.imshow(some_digit, cmap = \"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(y_test[i], class_names[y_test[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ecda3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47915708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking only label (1, 7)->0 & 9->1\n",
    "idx_1 = []\n",
    "# idx_7 = []\n",
    "idx_9 = []\n",
    "for i in range(len(y_test)):\n",
    "    if list(y_test)[i] == 1:\n",
    "        idx_1.append(i)\n",
    "#     if list(y_test)[i] == 7:\n",
    "#         idx_7.append(i)\n",
    "    if list(y_test)[i] == 9:\n",
    "        idx_9.append(i)\n",
    "print(len(idx_1), len(idx_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48014ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx_1[:50] + idx_9[:50]\n",
    "idx.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in idx:\n",
    "    X.append(X_test[i])\n",
    "    y.append(y_test[i])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6fca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653bdad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hard = [0.00 if x==1 else x for x in y]\n",
    "y_hard = [1.00 if x==9 else x for x in y_hard]\n",
    "pd.Series(y_hard).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "epochs = 10\n",
    "batch = 32\n",
    "decay=0.0001\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Dropout\n",
    "\n",
    "# transformer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "num_classes = 1\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "image_size = 54  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "    \n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes, activation='sigmoid')(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2036a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7496309",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hard = pd.DataFrame(y_hard)\n",
    "y_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5634c413",
   "metadata": {},
   "source": [
    "# 3-0. Generating Prob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = create_vit_classifier()   \n",
    "gen_model.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.0002), metrics=['accuracy'])\n",
    "history = gen_model.fit(X, y_hard, validation_split=0.2, epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "result = gen_model.predict(X, verbose=0)\n",
    "prob_label = list(result.reshape(len(X),))\n",
    "prob_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9740fa2",
   "metadata": {},
   "source": [
    "# 3-1. Focal(Hard) and SLS(Hard/diverse alphas)_option#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10):    # 10 times repeat    \n",
    "    res = pd.DataFrame({'Focal':[0, 0, 0, 0, 0]}, index = ['Acc','Pre','Rec','F1','R-AUC']) \n",
    "    # Focal\n",
    "    print('#'*50,'Focal','#'*50)\n",
    "    list_acc = []\n",
    "    list_pre = []\n",
    "    list_rec = []\n",
    "    list_f1 = []\n",
    "    list_rauc = []   \n",
    "    focal_model = create_vit_classifier()   \n",
    "\n",
    "    n_iter = 0\n",
    "    for train_index, test_index in skf.split(X, y_hard):  # straticiation by y_hard(binary label)\n",
    "        n_iter += 1\n",
    "        X_train = X[train_index]\n",
    "        y_train= y_hard.iloc[train_index]\n",
    "        if n_iter == 1:\n",
    "            print(y_train.value_counts())\n",
    "        X_test = X[test_index]\n",
    "        y_test= y_hard.iloc[test_index]\n",
    "#         print('#'*10,'{0}th CV'.format(n_iter),'#'*10)\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        y_train = y_train.astype(float)\n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "        y_test = y_test.astype(float)\n",
    "\n",
    "        focal_model.compile(loss='BinaryFocalCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.0002), metrics=['accuracy'])\n",
    "        history = focal_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        predicted = np.round(focal_model.predict(X_test, verbose=0))\n",
    "        list_acc.append(metrics.accuracy_score(y_test, predicted))\n",
    "        list_pre.append(metrics.precision_score(y_test, predicted))\n",
    "        list_rec.append(metrics.recall_score(y_test, predicted))\n",
    "        list_f1.append(metrics.f1_score(y_test, predicted))\n",
    "        list_rauc.append(metrics.roc_auc_score(y_test, predicted))\n",
    "    res['Focal'] = [np.mean(list_acc), np.mean(list_pre), np.mean(list_rec), np.mean(list_f1), np.mean(list_rauc)]\n",
    "    print([np.mean(list_acc), np.mean(list_pre), np.mean(list_rec), np.mean(list_f1), np.mean(list_rauc)])\n",
    "    \n",
    "    B = [0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10]  # SLS with LS\n",
    "    c = 0.1  # criterion decides easy/hard\n",
    "    for b in B:\n",
    "        print('#'*50,'SLS',b,'#'*50)\n",
    "        y_005 = []\n",
    "        for i in range(len(y_hard)):\n",
    "            if list(y_hard[0])[i] == 0:\n",
    "                if prob_label[i] <= c:\n",
    "                    y_005.append(b)  # easy sample\n",
    "                else:\n",
    "                    y_005.append(0) # (or 0-b) hard sample\n",
    "            if list(y_hard[0])[i] == 1:\n",
    "                if prob_label[i] >= 1-c:\n",
    "                    y_005.append(1-b)  # easy sample\n",
    "                else:\n",
    "                    y_005.append(1) # (or 1+b) hard sample\n",
    "        y_005 = pd.DataFrame(y_005)     \n",
    "\n",
    "        bce005_acc = []\n",
    "        bce005_pre = []\n",
    "        bce005_rec = []\n",
    "        bce005_f1 = []\n",
    "        bce005_rocauc = []\n",
    "        model_005 = create_vit_classifier()\n",
    "#             early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        n_iter = 0\n",
    "        for train_index, test_index in skf.split(X, y_hard):  # straticiation by y_hard(binary label)\n",
    "            n_iter += 1\n",
    "            X_train = X[train_index]\n",
    "            y_005_train= y_005.iloc[train_index]\n",
    "            if n_iter == 1:\n",
    "                print(y_005_train.value_counts())\n",
    "            X_test = X[test_index]\n",
    "            y_test= y_hard.iloc[test_index]  # test with real(actual) label y\n",
    "#                 print('#'*10,'{0}th CV'.format(n_iter),'#'*10)\n",
    "            X_train = np.array(X_train)\n",
    "            y_005_train = np.array(y_005_train)\n",
    "            y_005_train = y_005_train.astype(float)\n",
    "            X_test = np.array(X_test)\n",
    "            y_test = np.array(y_test)\n",
    "            y_test = y_test.astype(float)\n",
    "\n",
    "            # MLP_BCE(y_005)\n",
    "            model_005.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.0002), metrics=['accuracy'])\n",
    "            history = model_005.fit(X_train, y_005_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#             plt.plot(history.history['loss'], label='loss')\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "            predicted = np.round(model_005.predict(X_test, verbose=0))\n",
    "            bce005_acc.append(metrics.accuracy_score(y_test, predicted))\n",
    "            bce005_pre.append(metrics.precision_score(y_test, predicted))\n",
    "            bce005_rec.append(metrics.recall_score(y_test, predicted))\n",
    "            bce005_f1.append(metrics.f1_score(y_test, predicted))\n",
    "            bce005_rocauc.append(metrics.roc_auc_score(y_test, predicted))\n",
    "        res['SLS({})'.format(b)] = [np.mean(bce005_acc), np.mean(bce005_pre), np.mean(bce005_rec), np.mean(bce005_f1), np.mean(bce005_rocauc)]\n",
    "        print([np.mean(bce005_acc), np.mean(bce005_pre), np.mean(bce005_rec), np.mean(bce005_f1), np.mean(bce005_rocauc)])        \n",
    "    res.to_csv(\"ViT_CIFAR_10_5CV(SLS_opt#1_c0.1).csv\", mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53047ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote\n",
    "res = pd.read_csv(\"ViT_CIFAR_10_5CV(SLS_opt#1_c0.1).csv\")\n",
    "res = res.dropna(axis=0)\n",
    "res = res.rename(columns={'SLS(0.0)':'Hard'})\n",
    "df_acc = res[res.iloc[:,0] == 'Acc']\n",
    "df_acc = df_acc.reset_index(drop=True)\n",
    "df_acc = df_acc.iloc[:,1:].astype(float)\n",
    "col_name = df_acc.columns\n",
    "ave = []\n",
    "std = []\n",
    "for i in range(12):\n",
    "    ave.append(np.mean(list(df_acc.iloc[:,i])))\n",
    "    std.append(np.std(list(df_acc.iloc[:,i])))\n",
    "final = pd.DataFrame(ave, index=col_name, columns=[\"mean\"])\n",
    "final['std'] = std\n",
    "final.sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67085980",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2fb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f07c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
