{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94341590",
   "metadata": {},
   "source": [
    "## https://wikidocs.net/22891"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42ad73",
   "metadata": {},
   "source": [
    "# 1. Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
    "data = pd.read_csv(r'spam.csv', encoding='latin1')\n",
    "print('sample number:',len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ce1e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['Unnamed: 2']\n",
    "del data['Unnamed: 3']\n",
    "del data['Unnamed: 4']\n",
    "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2377a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing?\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique?\n",
    "data['v2'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e39e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete duplicate\n",
    "data.drop_duplicates(subset=['v2'], inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data = data[:1000]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f767e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['v1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd64731",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[:824][data['v1'][:824] == 0].index, inplace=True)\n",
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['v1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['v2']\n",
    "y = data['v1']\n",
    "y = y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e106ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24465a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encoding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_encoded = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index (more frequent, smaller number given)\n",
    "word_to_index = tokenizer.word_index\n",
    "# print(len(word_to_index), word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word group for padding\n",
    "vocab_size = len(word_to_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the longest email\n",
    "maxlen = 0\n",
    "for i in range(len(X_encoded)):\n",
    "    if len(X_encoded[i]) >= maxlen:\n",
    "        maxlen = len(X_encoded[i])\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "max_len = maxlen\n",
    "X_padded = pad_sequences(X_encoded, maxlen = max_len)\n",
    "X_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be62a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_padded\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hard = pd.DataFrame(y)\n",
    "y_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03204a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "epochs = 10\n",
    "batch = 64\n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, LSTM, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "embedding_dim = 32\n",
    "hidden_units = 32\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "    # model.add(SimpleRNN(hidden_units))\n",
    "    model.add(LSTM(hidden_units))\n",
    "    # model.add(GRU(hidden_units))\n",
    "    model.add(Dense(1, activation='sigmoid'))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112349ab",
   "metadata": {},
   "source": [
    "# 1-0. Generating Prob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9748a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_model = create_model()   \n",
    "gen_model.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.001), metrics=['accuracy'])\n",
    "history = gen_model.fit(X, y_hard, validation_split=0.2, epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "result = gen_model.predict(X, verbose=0)\n",
    "prob_label = list(result.reshape(len(X),))\n",
    "y = pd.DataFrame(prob_label)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe00d3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "c = 0.5\n",
    "y_edge = []\n",
    "for i in range(len(y)):\n",
    "    if list(y_hard['v1'])[i] == 0:\n",
    "        if list(y[0])[i] <= c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "    if list(y_hard['v1'])[i] == 1:\n",
    "        if list(y[0])[i] >= 1-c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "y_edge = pd.DataFrame(y_edge)\n",
    "y_edge.value_counts()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6cfc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "edge_list = list(y_edge[y_edge[0] == 2].index)\n",
    "normal_list = list(y_edge[y_edge[0] == 0].index)\n",
    "print(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdafabfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "print(r, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144680c",
   "metadata": {},
   "source": [
    "# 1-1. Focal(Hard) and SLS(Hard/alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca64cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "B = [0.00, alpha]\n",
    "\n",
    "for t in range(10):    # 10 times repeat      \n",
    "    res = pd.DataFrame({'Focal':[0, 0, 0]}, index = ['Total','Edge','Normal']) \n",
    "    # Focal\n",
    "    print('#'*50,'Focal','#'*50)\n",
    "    list_total = []\n",
    "    list_edge = []\n",
    "    list_normal = []  \n",
    "    focal_model = create_model()   \n",
    "\n",
    "    n_iter = 0\n",
    "    for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "        n_iter += 1\n",
    "        X_train = X[train_index]\n",
    "        y_train= y_hard.iloc[train_index]     # train with hard labels\n",
    "        if n_iter == 1:\n",
    "            print(y_train.value_counts())\n",
    "        X_test = X[test_index]\n",
    "        y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "        test_edge_list = []\n",
    "        for index in edge_list:\n",
    "            if index in test_index:\n",
    "                test_edge_list.append(index)\n",
    "        X_test_edge = X[test_edge_list]\n",
    "        y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "        test_normal_list = []\n",
    "        for index in normal_list:\n",
    "            if index in test_index:\n",
    "                test_normal_list.append(index)\n",
    "        X_test_normal = X[test_normal_list]\n",
    "        y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        y_train = y_train.astype(float)    \n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "        y_test = y_test.astype(float)\n",
    "        X_test_edge = np.array(X_test_edge)\n",
    "        y_test_edge = np.array(y_test_edge)\n",
    "        y_test_edge = y_test_edge.astype(float)\n",
    "        X_test_normal = np.array(X_test_normal)\n",
    "        y_test_normal = np.array(y_test_normal)\n",
    "        y_test_normal = y_test_normal.astype(float)\n",
    "\n",
    "        focal_model.compile(loss='BinaryFocalCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.001), metrics=['accuracy'])\n",
    "        history = focal_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "        # TEST (total)\n",
    "        predicted_total = np.round(focal_model.predict(X_test, verbose=0))\n",
    "        list_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "        # TEST (edge)\n",
    "        predicted_edge = np.round(focal_model.predict(X_test_edge, verbose=0))\n",
    "        list_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "        # TEST (normal)\n",
    "        predicted_normal = np.round(focal_model.predict(X_test_normal, verbose=0))\n",
    "        list_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "            \n",
    "    res['Focal'] = [np.mean(list_total), np.mean(list_edge), np.mean(list_normal)]\n",
    "    print([np.mean(list_total), np.mean(list_edge), np.mean(list_normal)])\n",
    "    \n",
    "    for b in B:\n",
    "        print('#'*50,'SLS',b,'#'*50)\n",
    "        y_sls = []\n",
    "        for i in range(len(y_hard)):\n",
    "            if list(y_hard['v1'])[i] == 0:\n",
    "                if prob_label[i] <= c:\n",
    "                    y_sls.append(b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(0) # hard sample\n",
    "            if list(y_hard['v1'])[i] == 1:\n",
    "                if prob_label[i] >= 1-c:\n",
    "                    y_sls.append(1-b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(1) # hard sample\n",
    "        y_sls = pd.DataFrame(y_sls)       \n",
    "\n",
    "        sls_total = []\n",
    "        sls_edge = []\n",
    "        sls_normal = []\n",
    "        model_sls = create_model()\n",
    "\n",
    "        n_iter = 0\n",
    "        for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "            n_iter += 1\n",
    "            X_train = X[train_index]\n",
    "            y_sls_train = y_sls.iloc[train_index]     # train with sls labels\n",
    "            if n_iter == 1:\n",
    "                print(y_sls_train.value_counts())\n",
    "            X_test = X[test_index]\n",
    "            y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "            test_edge_list = []\n",
    "            for index in edge_list:\n",
    "                if index in test_index:\n",
    "                    test_edge_list.append(index)\n",
    "            X_test_edge = X[test_edge_list]\n",
    "            y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "            test_normal_list = []\n",
    "            for index in normal_list:\n",
    "                if index in test_index:\n",
    "                    test_normal_list.append(index)\n",
    "            X_test_normal = X[test_normal_list]\n",
    "            y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            y_sls_train = np.array(y_sls_train)\n",
    "            y_sls_train = y_sls_train.astype(float)\n",
    "            X_test = np.array(X_test)\n",
    "            y_test = np.array(y_test)\n",
    "            y_test = y_test.astype(float)\n",
    "            X_test_edge = np.array(X_test_edge)\n",
    "            y_test_edge = np.array(y_test_edge)\n",
    "            y_test_edge = y_test_edge.astype(float)\n",
    "            X_test_normal = np.array(X_test_normal)\n",
    "            y_test_normal = np.array(y_test_normal)\n",
    "            y_test_normal = y_test_normal.astype(float)\n",
    "\n",
    "            # MLP_BCE(y_005)\n",
    "            model_sls.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.001), metrics=['accuracy'])\n",
    "            history = model_sls.fit(X_train, y_sls_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#             plt.plot(history.history['loss'], label='loss')\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "            \n",
    "            # TEST (total)\n",
    "            predicted_total = np.round(model_sls.predict(X_test, verbose=0))\n",
    "            sls_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "            # TEST (edge)\n",
    "            predicted_edge = np.round(model_sls.predict(X_test_edge, verbose=0))\n",
    "            sls_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "            # TEST (normal)\n",
    "            predicted_normal = np.round(model_sls.predict(X_test_normal, verbose=0))\n",
    "            sls_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "                       \n",
    "        res['SLS({})'.format(b)] = [np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)]\n",
    "        print([np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)])         \n",
    "    res.to_csv(\"RNN_SPAM_5CV(SLS_c0.5)_alphaimproved.csv\", mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f491103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0e7257c",
   "metadata": {},
   "source": [
    "# 2. Reuters News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96858ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2886788",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "\n",
    "(XX, yy), (X_dummy, y_dummy) = reuters.load_data(num_words=vocab_size, test_split=0)\n",
    "\n",
    "print(len(XX))\n",
    "print(len(X_dummy))\n",
    "num_classes = len(set(yy))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(XX[0])\n",
    "print(yy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405db59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = reuters.get_word_index()\n",
    "# print(len(word_to_index), word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word group for padding\n",
    "# vocab_size = len(word_to_index) + 3\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bdb95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the longest news\n",
    "maxlen = 0\n",
    "for i in range(len(XX)):\n",
    "    if len(XX[i]) >= maxlen:\n",
    "        maxlen = len(XX[i])\n",
    "        \n",
    "# maxlen = 100\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3643e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "max_len = maxlen\n",
    "X_padded = pad_sequences(XX, maxlen = max_len)\n",
    "X_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee0172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(yy).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f07221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking only label 3->'0' & 1->'1'\n",
    "idx_4 = []\n",
    "idx_3 = []\n",
    "for i in range(len(yy)):\n",
    "    if list(yy)[i] == 4:\n",
    "        idx_4.append(i)\n",
    "    if list(yy)[i] == 3:\n",
    "        idx_3.append(i)\n",
    "print(len(idx_4), len(idx_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx_4[:150] + idx_3[:150]\n",
    "idx.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d99bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in idx:\n",
    "    X.append(X_padded[i])\n",
    "    y.append(yy[i])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647797c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hard = [0.00 if x==3 else x for x in y]\n",
    "y_hard = [1.00 if x==4 else x for x in y_hard]\n",
    "pd.Series(y_hard).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c258740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "epochs = 10\n",
    "batch = 64\n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, LSTM, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "embedding_dim = 32\n",
    "hidden_units = 32\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "    # model.add(SimpleRNN(hidden_units))\n",
    "    model.add(LSTM(hidden_units))\n",
    "    # model.add(GRU(hidden_units))\n",
    "    model.add(Dense(1, activation='sigmoid'))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a23adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e7229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hard = pd.DataFrame(y_hard)\n",
    "y_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8e995",
   "metadata": {},
   "source": [
    "# 2-0. Generating Prob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302db28c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_model = create_model()   \n",
    "gen_model.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.005), metrics=['accuracy'])\n",
    "history = gen_model.fit(X, y_hard, validation_split=0.2, epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "result = gen_model.predict(X, verbose=0)\n",
    "prob_label = list(result.reshape(len(X),))\n",
    "y = pd.DataFrame(prob_label)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "c = 0.5\n",
    "y_edge = []\n",
    "for i in range(len(y)):\n",
    "    if list(y_hard[0])[i] == 0:\n",
    "        if list(y[0])[i] <= c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "    if list(y_hard[0])[i] == 1:\n",
    "        if list(y[0])[i] >= 1-c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "y_edge = pd.DataFrame(y_edge)\n",
    "y_edge.value_counts()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a13eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "edge_list = list(y_edge[y_edge[0] == 2].index)\n",
    "normal_list = list(y_edge[y_edge[0] == 0].index)\n",
    "print(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4931fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "print(r, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d02473",
   "metadata": {},
   "source": [
    "# 2-1. Focal(Hard) and SLS(Hard/alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7dfb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "B = [0.00, alpha]\n",
    "\n",
    "for t in range(10):    # 10 times repeat      \n",
    "    res = pd.DataFrame({'Focal':[0, 0, 0]}, index = ['Total','Edge','Normal']) \n",
    "    # Focal\n",
    "    print('#'*50,'Focal','#'*50)\n",
    "    list_total = []\n",
    "    list_edge = []\n",
    "    list_normal = []  \n",
    "    focal_model = create_model()   \n",
    "\n",
    "    n_iter = 0\n",
    "    for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "        n_iter += 1\n",
    "        X_train = X[train_index]\n",
    "        y_train= y_hard.iloc[train_index]     # train with hard labels\n",
    "        if n_iter == 1:\n",
    "            print(y_train.value_counts())\n",
    "        X_test = X[test_index]\n",
    "        y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "        test_edge_list = []\n",
    "        for index in edge_list:\n",
    "            if index in test_index:\n",
    "                test_edge_list.append(index)\n",
    "        X_test_edge = X[test_edge_list]\n",
    "        y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "        test_normal_list = []\n",
    "        for index in normal_list:\n",
    "            if index in test_index:\n",
    "                test_normal_list.append(index)\n",
    "        X_test_normal = X[test_normal_list]\n",
    "        y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        y_train = y_train.astype(float)    \n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "        y_test = y_test.astype(float)\n",
    "        X_test_edge = np.array(X_test_edge)\n",
    "        y_test_edge = np.array(y_test_edge)\n",
    "        y_test_edge = y_test_edge.astype(float)\n",
    "        X_test_normal = np.array(X_test_normal)\n",
    "        y_test_normal = np.array(y_test_normal)\n",
    "        y_test_normal = y_test_normal.astype(float)\n",
    "\n",
    "        focal_model.compile(loss='BinaryFocalCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.005), metrics=['accuracy'])\n",
    "        history = focal_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "        # TEST (total)\n",
    "        predicted_total = np.round(focal_model.predict(X_test, verbose=0))\n",
    "        list_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "        # TEST (edge)\n",
    "        predicted_edge = np.round(focal_model.predict(X_test_edge, verbose=0))\n",
    "        list_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "        # TEST (normal)\n",
    "        predicted_normal = np.round(focal_model.predict(X_test_normal, verbose=0))\n",
    "        list_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "            \n",
    "    res['Focal'] = [np.mean(list_total), np.mean(list_edge), np.mean(list_normal)]\n",
    "    print([np.mean(list_total), np.mean(list_edge), np.mean(list_normal)])\n",
    "    \n",
    "    for b in B:\n",
    "        print('#'*50,'SLS',b,'#'*50)\n",
    "        y_sls = []\n",
    "        for i in range(len(y_hard)):\n",
    "            if list(y_hard[0])[i] == 0:\n",
    "                if prob_label[i] <= c:\n",
    "                    y_sls.append(b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(0) # hard sample\n",
    "            if list(y_hard[0])[i] == 1:\n",
    "                if prob_label[i] >= 1-c:\n",
    "                    y_sls.append(1-b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(1) # hard sample\n",
    "        y_sls = pd.DataFrame(y_sls)       \n",
    "\n",
    "        sls_total = []\n",
    "        sls_edge = []\n",
    "        sls_normal = []\n",
    "        model_sls = create_model()\n",
    "\n",
    "        n_iter = 0\n",
    "        for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "            n_iter += 1\n",
    "            X_train = X[train_index]\n",
    "            y_sls_train = y_sls.iloc[train_index]     # train with sls labels\n",
    "            if n_iter == 1:\n",
    "                print(y_sls_train.value_counts())\n",
    "            X_test = X[test_index]\n",
    "            y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "            test_edge_list = []\n",
    "            for index in edge_list:\n",
    "                if index in test_index:\n",
    "                    test_edge_list.append(index)\n",
    "            X_test_edge = X[test_edge_list]\n",
    "            y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "            test_normal_list = []\n",
    "            for index in normal_list:\n",
    "                if index in test_index:\n",
    "                    test_normal_list.append(index)\n",
    "            X_test_normal = X[test_normal_list]\n",
    "            y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            y_sls_train = np.array(y_sls_train)\n",
    "            y_sls_train = y_sls_train.astype(float)\n",
    "            X_test = np.array(X_test)\n",
    "            y_test = np.array(y_test)\n",
    "            y_test = y_test.astype(float)\n",
    "            X_test_edge = np.array(X_test_edge)\n",
    "            y_test_edge = np.array(y_test_edge)\n",
    "            y_test_edge = y_test_edge.astype(float)\n",
    "            X_test_normal = np.array(X_test_normal)\n",
    "            y_test_normal = np.array(y_test_normal)\n",
    "            y_test_normal = y_test_normal.astype(float)\n",
    "\n",
    "            # MLP_BCE(y_005)\n",
    "            model_sls.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.005), metrics=['accuracy'])\n",
    "            history = model_sls.fit(X_train, y_sls_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#             plt.plot(history.history['loss'], label='loss')\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "            \n",
    "            # TEST (total)\n",
    "            predicted_total = np.round(model_sls.predict(X_test, verbose=0))\n",
    "            sls_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "            # TEST (edge)\n",
    "            predicted_edge = np.round(model_sls.predict(X_test_edge, verbose=0))\n",
    "            sls_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "            # TEST (normal)\n",
    "            predicted_normal = np.round(model_sls.predict(X_test_normal, verbose=0))\n",
    "            sls_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "                       \n",
    "        res['SLS({})'.format(b)] = [np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)]\n",
    "        print([np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)])         \n",
    "    res.to_csv(\"RNN_RNEWS_5CV(SLS_c0.5)_alphaimproved.csv.csv\", mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491e7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33136bf8",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_len = 100\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e380edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making lists of index\n",
    "idx_0 = []\n",
    "idx_1 = []\n",
    "for i in range(len(y_train)):\n",
    "    if list(y_train)[i] == 0:\n",
    "        idx_0.append(i)\n",
    "    if list(y_train)[i] == 1:\n",
    "        idx_1.append(i)\n",
    "print(len(idx_0), len(idx_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx_0[:150] + idx_1[:150]\n",
    "idx.sort()\n",
    "# print(len(idx), idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in idx:\n",
    "    X.append(X_train[i])\n",
    "    y.append(y_train[i])\n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2238c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = imdb.get_word_index()\n",
    "# print(len(word_to_index), word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "X_padded = pad_sequences(X, maxlen = max_len)\n",
    "X_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a65e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "epochs = 10\n",
    "batch = 64\n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, LSTM, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "embedding_dim = 32\n",
    "hidden_units = 32\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "    # model.add(SimpleRNN(hidden_units))\n",
    "    model.add(LSTM(hidden_units))\n",
    "    # model.add(GRU(hidden_units))\n",
    "    model.add(Dense(1, activation='sigmoid'))    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_padded\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c7889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hard = pd.DataFrame(y_hard)\n",
    "y_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4641a",
   "metadata": {},
   "source": [
    "# 3-0. Generating Prob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35263195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_model = create_model()   \n",
    "gen_model.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.005), metrics=['accuracy'])\n",
    "history = gen_model.fit(X, y_hard, validation_split=0.2, epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "result = gen_model.predict(X, verbose=0)\n",
    "prob_label = list(result.reshape(len(X),))\n",
    "y = pd.DataFrame(prob_label)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809baf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "c = 0.5\n",
    "y_edge = []\n",
    "for i in range(len(y)):\n",
    "    if list(y_hard[0])[i] == 0:\n",
    "        if list(y[0])[i] <= c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "    if list(y_hard[0])[i] == 1:\n",
    "        if list(y[0])[i] >= 1-c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "y_edge = pd.DataFrame(y_edge)\n",
    "y_edge.value_counts()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ae20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "edge_list = list(y_edge[y_edge[0] == 2].index)\n",
    "normal_list = list(y_edge[y_edge[0] == 0].index)\n",
    "print(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "print(r, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88447e2a",
   "metadata": {},
   "source": [
    "# 3-1. Focal(Hard) and SLS(Hard/alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd1b93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "B = [0.00, alpha]\n",
    "\n",
    "for t in range(10):    # 10 times repeat      \n",
    "    res = pd.DataFrame({'Focal':[0, 0, 0]}, index = ['Total','Edge','Normal']) \n",
    "    # Focal\n",
    "    print('#'*50,'Focal','#'*50)\n",
    "    list_total = []\n",
    "    list_edge = []\n",
    "    list_normal = []  \n",
    "    focal_model = create_model()   \n",
    "\n",
    "    n_iter = 0\n",
    "    for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "        n_iter += 1\n",
    "        X_train = X[train_index]\n",
    "        y_train= y_hard.iloc[train_index]     # train with hard labels\n",
    "        if n_iter == 1:\n",
    "            print(y_train.value_counts())\n",
    "        X_test = X[test_index]\n",
    "        y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "        test_edge_list = []\n",
    "        for index in edge_list:\n",
    "            if index in test_index:\n",
    "                test_edge_list.append(index)\n",
    "        X_test_edge = X[test_edge_list]\n",
    "        y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "        test_normal_list = []\n",
    "        for index in normal_list:\n",
    "            if index in test_index:\n",
    "                test_normal_list.append(index)\n",
    "        X_test_normal = X[test_normal_list]\n",
    "        y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        y_train = y_train.astype(float)    \n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "        y_test = y_test.astype(float)\n",
    "        X_test_edge = np.array(X_test_edge)\n",
    "        y_test_edge = np.array(y_test_edge)\n",
    "        y_test_edge = y_test_edge.astype(float)\n",
    "        X_test_normal = np.array(X_test_normal)\n",
    "        y_test_normal = np.array(y_test_normal)\n",
    "        y_test_normal = y_test_normal.astype(float)\n",
    "\n",
    "        focal_model.compile(loss='BinaryFocalCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.005), metrics=['accuracy'])\n",
    "        history = focal_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "        # TEST (total)\n",
    "        predicted_total = np.round(focal_model.predict(X_test, verbose=0))\n",
    "        list_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "        # TEST (edge)\n",
    "        predicted_edge = np.round(focal_model.predict(X_test_edge, verbose=0))\n",
    "        list_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "        # TEST (normal)\n",
    "        predicted_normal = np.round(focal_model.predict(X_test_normal, verbose=0))\n",
    "        list_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "            \n",
    "    res['Focal'] = [np.mean(list_total), np.mean(list_edge), np.mean(list_normal)]\n",
    "    print([np.mean(list_total), np.mean(list_edge), np.mean(list_normal)])\n",
    "    \n",
    "    for b in B:\n",
    "        print('#'*50,'SLS',b,'#'*50)\n",
    "        y_sls = []\n",
    "        for i in range(len(y_hard)):\n",
    "            if list(y_hard[0])[i] == 0:\n",
    "                if prob_label[i] <= c:\n",
    "                    y_sls.append(b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(0) # hard sample\n",
    "            if list(y_hard[0])[i] == 1:\n",
    "                if prob_label[i] >= 1-c:\n",
    "                    y_sls.append(1-b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(1) # hard sample\n",
    "        y_sls = pd.DataFrame(y_sls)       \n",
    "\n",
    "        sls_total = []\n",
    "        sls_edge = []\n",
    "        sls_normal = []\n",
    "        model_sls = create_model()\n",
    "\n",
    "        n_iter = 0\n",
    "        for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "            n_iter += 1\n",
    "            X_train = X[train_index]\n",
    "            y_sls_train = y_sls.iloc[train_index]     # train with sls labels\n",
    "            if n_iter == 1:\n",
    "                print(y_sls_train.value_counts())\n",
    "            X_test = X[test_index]\n",
    "            y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "            test_edge_list = []\n",
    "            for index in edge_list:\n",
    "                if index in test_index:\n",
    "                    test_edge_list.append(index)\n",
    "            X_test_edge = X[test_edge_list]\n",
    "            y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "            test_normal_list = []\n",
    "            for index in normal_list:\n",
    "                if index in test_index:\n",
    "                    test_normal_list.append(index)\n",
    "            X_test_normal = X[test_normal_list]\n",
    "            y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            y_sls_train = np.array(y_sls_train)\n",
    "            y_sls_train = y_sls_train.astype(float)\n",
    "            X_test = np.array(X_test)\n",
    "            y_test = np.array(y_test)\n",
    "            y_test = y_test.astype(float)\n",
    "            X_test_edge = np.array(X_test_edge)\n",
    "            y_test_edge = np.array(y_test_edge)\n",
    "            y_test_edge = y_test_edge.astype(float)\n",
    "            X_test_normal = np.array(X_test_normal)\n",
    "            y_test_normal = np.array(y_test_normal)\n",
    "            y_test_normal = y_test_normal.astype(float)\n",
    "\n",
    "            # MLP_BCE(y_005)\n",
    "            model_sls.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.005), metrics=['accuracy'])\n",
    "            history = model_sls.fit(X_train, y_sls_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#             plt.plot(history.history['loss'], label='loss')\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "            \n",
    "            # TEST (total)\n",
    "            predicted_total = np.round(model_sls.predict(X_test, verbose=0))\n",
    "            sls_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "            # TEST (edge)\n",
    "            predicted_edge = np.round(model_sls.predict(X_test_edge, verbose=0))\n",
    "            sls_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "            # TEST (normal)\n",
    "            predicted_normal = np.round(model_sls.predict(X_test_normal, verbose=0))\n",
    "            sls_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "                       \n",
    "        res['SLS({})'.format(b)] = [np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)]\n",
    "        print([np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)])         \n",
    "    res.to_csv(\"RNN_IMDB_5CV(SLS_c0.5)_alphaimproved.csv\", mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d2fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337422e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
