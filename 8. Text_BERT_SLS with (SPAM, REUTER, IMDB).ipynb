{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e427e5",
   "metadata": {},
   "source": [
    "# 1. Sapm Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b890c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
    "data = pd.read_csv(r'spam.csv', encoding='latin1')\n",
    "print('sample number:',len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['Unnamed: 2']\n",
    "del data['Unnamed: 3']\n",
    "del data['Unnamed: 4']\n",
    "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['v2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e19414",
   "metadata": {},
   "outputs": [],
   "source": [
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data = data[:200]\n",
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data\n",
    "\n",
    "# numlist = list(range(len(data)))\n",
    "# data = data.sample(400, random_state=100)\n",
    "# numlist = list(range(len(data)))\n",
    "# data = data.set_index(pd.Index(numlist))\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bdb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['v1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997397e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.drop(data[:380][data['v1'][:380] == 0].index, inplace=True)\n",
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc081537",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['v2']\n",
    "y = data['v1']\n",
    "y = y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf02629",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X), X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tf.convert_to_tensor(y, dtype=tf.float32, dtype_hint=None, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb955fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y), y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "#     net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)     # activation=None\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "batch = 32\n",
    "epochs = 10\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "steps_per_epoch = int(len(X)*0.4)  # length of train data\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf051f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dbea96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hard = pd.DataFrame(y)\n",
    "y_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5eece",
   "metadata": {},
   "source": [
    "# 1-0. Generating Prob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8ccc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_gen = tf.convert_to_tensor(X, dtype=tf.string, dtype_hint=None, name=None)\n",
    "gen_model = build_classifier_model()\n",
    "opt = optimization.create_optimizer(init_lr=0.0001,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')\n",
    "gen_model.compile(loss='BinaryCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "history = gen_model.fit(X_gen, y_hard, validation_split=0.2, epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "result = gen_model.predict(X_gen, verbose=0)\n",
    "prob_label = list(result.reshape(len(X),))\n",
    "y = pd.DataFrame(prob_label)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de21d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "c = 0.5\n",
    "y_edge = []\n",
    "for i in range(len(y)):\n",
    "    if list(y_hard['v1'])[i] == 0:\n",
    "        if list(y[0])[i] <= c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "    if list(y_hard['v1'])[i] == 1:\n",
    "        if list(y[0])[i] >= 1-c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "y_edge = pd.DataFrame(y_edge)\n",
    "y_edge.value_counts()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "edge_list = list(y_edge[y_edge[0] == 2].index)\n",
    "normal_list = list(y_edge[y_edge[0] == 0].index)\n",
    "print(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a3abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "print(r, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa6e8b",
   "metadata": {},
   "source": [
    "# 1-1. Focal(Hard) and SLS(Hard/alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35767a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "B = [0.00, alpha]\n",
    "\n",
    "for t in range(10):    # 10 times repeat      \n",
    "    res = pd.DataFrame({'Focal':[0, 0, 0]}, index = ['Total','Edge','Normal']) \n",
    "    # Focal\n",
    "    print('#'*50,'Focal','#'*50)\n",
    "    list_total = []\n",
    "    list_edge = []\n",
    "    list_normal = []  \n",
    "    focal_model = build_classifier_model()   \n",
    "\n",
    "    n_iter = 0\n",
    "    for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "        n_iter += 1\n",
    "        X_train = X[train_index]\n",
    "        y_train= y_hard.iloc[train_index]     # train with hard labels\n",
    "        if n_iter == 1:\n",
    "            print(y_train.value_counts())\n",
    "        X_test = X[test_index]\n",
    "        y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "        test_edge_list = []\n",
    "        for index in edge_list:\n",
    "            if index in test_index:\n",
    "                test_edge_list.append(index)\n",
    "        X_test_edge = X[test_edge_list]\n",
    "        y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "        test_normal_list = []\n",
    "        for index in normal_list:\n",
    "            if index in test_index:\n",
    "                test_normal_list.append(index)\n",
    "        X_test_normal = X[test_normal_list]\n",
    "        y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "        \n",
    "        X_train = tf.convert_to_tensor(X_train, dtype=tf.string, dtype_hint=None, name=None)\n",
    "        X_test = tf.convert_to_tensor(X_test, dtype=tf.string, dtype_hint=None, name=None)\n",
    "        X_test_edge = tf.convert_to_tensor(X_test_edge, dtype=tf.string, dtype_hint=None, name=None)\n",
    "        X_test_normal = tf.convert_to_tensor(X_test_normal, dtype=tf.string, dtype_hint=None, name=None)\n",
    "\n",
    "        opt = optimization.create_optimizer(init_lr=0.0001,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')\n",
    "        focal_model.compile(loss='BinaryFocalCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        history = focal_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "        # TEST (total)\n",
    "        predicted_total = np.round(focal_model.predict(X_test, verbose=0))\n",
    "        list_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "        # TEST (edge)\n",
    "        predicted_edge = np.round(focal_model.predict(X_test_edge, verbose=0))\n",
    "        list_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "        # TEST (normal)\n",
    "        predicted_normal = np.round(focal_model.predict(X_test_normal, verbose=0))\n",
    "        list_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "            \n",
    "    res['Focal'] = [np.mean(list_total), np.mean(list_edge), np.mean(list_normal)]\n",
    "    print([np.mean(list_total), np.mean(list_edge), np.mean(list_normal)])\n",
    "\n",
    "    for b in B:\n",
    "        print('#'*50,'SLS',b,'#'*50)\n",
    "        y_sls = []\n",
    "        for i in range(len(y_hard)):\n",
    "            if list(y_hard['v1'])[i] == 0:\n",
    "                if prob_label[i] <= c:\n",
    "                    y_sls.append(b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(0) # (or 0-b) hard sample\n",
    "            if list(y_hard['v1'])[i] == 1:\n",
    "                if prob_label[i] >= 1-c:\n",
    "                    y_sls.append(1-b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(1) # (or 1+b) hard sample\n",
    "        y_sls = pd.DataFrame(y_sls)     \n",
    "\n",
    "        sls_total = []\n",
    "        sls_edge = []\n",
    "        sls_normal = []\n",
    "        model_sls = build_classifier_model() \n",
    "\n",
    "        n_iter = 0\n",
    "        for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "            n_iter += 1\n",
    "            X_train = X[train_index]\n",
    "            y_sls_train = y_sls.iloc[train_index]     # train with sls labels\n",
    "            if n_iter == 1:\n",
    "                print(y_sls_train.value_counts())\n",
    "            X_test = X[test_index]\n",
    "            y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "            test_edge_list = []\n",
    "            for index in edge_list:\n",
    "                if index in test_index:\n",
    "                    test_edge_list.append(index)\n",
    "            X_test_edge = X[test_edge_list]\n",
    "            y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "            test_normal_list = []\n",
    "            for index in normal_list:\n",
    "                if index in test_index:\n",
    "                    test_normal_list.append(index)\n",
    "            X_test_normal = X[test_normal_list]\n",
    "            y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "\n",
    "            X_train = tf.convert_to_tensor(X_train, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            X_test = tf.convert_to_tensor(X_test, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            X_test_edge = tf.convert_to_tensor(X_test_edge, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            X_test_normal = tf.convert_to_tensor(X_test_normal, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            \n",
    "            # MLP_BCE(y_005)\n",
    "            opt = optimization.create_optimizer(init_lr=0.0001,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')\n",
    "            model_sls.compile(loss='BinaryCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "            history = model_sls.fit(X_train, y_sls_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#             plt.plot(history.history['loss'], label='loss')\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "            \n",
    "            # TEST (total)\n",
    "            predicted_total = np.round(model_sls.predict(X_test, verbose=0))\n",
    "            sls_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "            # TEST (edge)\n",
    "            predicted_edge = np.round(model_sls.predict(X_test_edge, verbose=0))\n",
    "            sls_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "            # TEST (normal)\n",
    "            predicted_normal = np.round(model_sls.predict(X_test_normal, verbose=0))\n",
    "            sls_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "                       \n",
    "        res['SLS({})'.format(b)] = [np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)]\n",
    "        print([np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)])          \n",
    "    res.to_csv(\"BERT_SPAM_5CV(SLS_c0.5)_alphaimproved.csv\", mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf2b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57d0bd68",
   "metadata": {},
   "source": [
    "# 2. Reuters News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e9571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ModHayes_train.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"text\",\"topics\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1271a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topics'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1009f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isblank = data['topics'] == \"[]\"\n",
    "isacq = data['topics'] == \"['acq']\"\n",
    "data = data[isblank | isacq]\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae7e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['topics'] = data['topics'].replace([\"[]\",\"['acq']\"],[0,1])\n",
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data = data[:350]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43acf67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topics'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f8bea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.drop(data[:295][data['topics'][:295] == 0].index, inplace=True)\n",
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ac780",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topics'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "y = data['topics']\n",
    "y = y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad720b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ec40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "#     net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)     # activation=None\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58add176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "batch = 64\n",
    "epochs = 10\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "steps_per_epoch = int(len(X)*0.4)  # length of train data\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b8ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f091df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hard = pd.DataFrame(y)\n",
    "y_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0392cf",
   "metadata": {},
   "source": [
    "# 2-0. Generating Prob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a68be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_gen = tf.convert_to_tensor(X, dtype=tf.string, dtype_hint=None, name=None)\n",
    "gen_model = build_classifier_model()\n",
    "opt = optimization.create_optimizer(init_lr=0.0002,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')\n",
    "gen_model.compile(loss='BinaryCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "history = gen_model.fit(X_gen, y_hard, validation_split=0.2, epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "result = gen_model.predict(X_gen, verbose=0)\n",
    "prob_label = list(result.reshape(len(X),))\n",
    "y = pd.DataFrame(prob_label)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "c = 0.5\n",
    "y_edge = []\n",
    "for i in range(len(y)):\n",
    "    if list(y_hard['topics'])[i] == 0:\n",
    "        if list(y[0])[i] <= c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "    if list(y_hard['topics'])[i] == 1:\n",
    "        if list(y[0])[i] >= 1-c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "y_edge = pd.DataFrame(y_edge)\n",
    "y_edge.value_counts()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea920b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "edge_list = list(y_edge[y_edge[0] == 2].index)\n",
    "normal_list = list(y_edge[y_edge[0] == 0].index)\n",
    "print(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e331c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "print(r, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55815a56",
   "metadata": {},
   "source": [
    "# 2-1. Focal(Hard) and SLS(Hard/alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed4a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "B = [0.00, alpha]\n",
    "\n",
    "for t in range(10):    # 10 times repeat      \n",
    "    res = pd.DataFrame({'Focal':[0, 0, 0]}, index = ['Total','Edge','Normal']) \n",
    "    # Focal\n",
    "    print('#'*50,'Focal','#'*50)\n",
    "    list_total = []\n",
    "    list_edge = []\n",
    "    list_normal = []  \n",
    "    focal_model = build_classifier_model()   \n",
    "\n",
    "    n_iter = 0\n",
    "    for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "        n_iter += 1\n",
    "        X_train = X[train_index]\n",
    "        y_train= y_hard.iloc[train_index]     # train with hard labels\n",
    "        if n_iter == 1:\n",
    "            print(y_train.value_counts())\n",
    "        X_test = X[test_index]\n",
    "        y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "        test_edge_list = []\n",
    "        for index in edge_list:\n",
    "            if index in test_index:\n",
    "                test_edge_list.append(index)\n",
    "        X_test_edge = X[test_edge_list]\n",
    "        y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "        test_normal_list = []\n",
    "        for index in normal_list:\n",
    "            if index in test_index:\n",
    "                test_normal_list.append(index)\n",
    "        X_test_normal = X[test_normal_list]\n",
    "        y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "        \n",
    "        X_train = tf.convert_to_tensor(X_train, dtype=tf.string, dtype_hint=None, name=None)\n",
    "        X_test = tf.convert_to_tensor(X_test, dtype=tf.string, dtype_hint=None, name=None)\n",
    "        X_test_edge = tf.convert_to_tensor(X_test_edge, dtype=tf.string, dtype_hint=None, name=None)\n",
    "        X_test_normal = tf.convert_to_tensor(X_test_normal, dtype=tf.string, dtype_hint=None, name=None)\n",
    "\n",
    "        opt = optimization.create_optimizer(init_lr=0.0002,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')\n",
    "        focal_model.compile(loss='BinaryFocalCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        history = focal_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "        # TEST (total)\n",
    "        predicted_total = np.round(focal_model.predict(X_test, verbose=0))\n",
    "        list_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "        # TEST (edge)\n",
    "        predicted_edge = np.round(focal_model.predict(X_test_edge, verbose=0))\n",
    "        list_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "        # TEST (normal)\n",
    "        predicted_normal = np.round(focal_model.predict(X_test_normal, verbose=0))\n",
    "        list_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "            \n",
    "    res['Focal'] = [np.mean(list_total), np.mean(list_edge), np.mean(list_normal)]\n",
    "    print([np.mean(list_total), np.mean(list_edge), np.mean(list_normal)])\n",
    "\n",
    "    for b in B:\n",
    "        print('#'*50,'SLS',b,'#'*50)\n",
    "        y_sls = []\n",
    "        for i in range(len(y_hard)):\n",
    "            if list(y_hard['topics'])[i] == 0:\n",
    "                if prob_label[i] <= c:\n",
    "                    y_sls.append(b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(0) # (or 0-b) hard sample\n",
    "            if list(y_hard['topics'])[i] == 1:\n",
    "                if prob_label[i] >= 1-c:\n",
    "                    y_sls.append(1-b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(1) # (or 1+b) hard sample\n",
    "        y_sls = pd.DataFrame(y_sls)     \n",
    "\n",
    "        sls_total = []\n",
    "        sls_edge = []\n",
    "        sls_normal = []\n",
    "        model_sls = build_classifier_model() \n",
    "\n",
    "        n_iter = 0\n",
    "        for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "            n_iter += 1\n",
    "            X_train = X[train_index]\n",
    "            y_sls_train = y_sls.iloc[train_index]     # train with sls labels\n",
    "            if n_iter == 1:\n",
    "                print(y_sls_train.value_counts())\n",
    "            X_test = X[test_index]\n",
    "            y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "            test_edge_list = []\n",
    "            for index in edge_list:\n",
    "                if index in test_index:\n",
    "                    test_edge_list.append(index)\n",
    "            X_test_edge = X[test_edge_list]\n",
    "            y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "            test_normal_list = []\n",
    "            for index in normal_list:\n",
    "                if index in test_index:\n",
    "                    test_normal_list.append(index)\n",
    "            X_test_normal = X[test_normal_list]\n",
    "            y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "\n",
    "            X_train = tf.convert_to_tensor(X_train, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            X_test = tf.convert_to_tensor(X_test, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            X_test_edge = tf.convert_to_tensor(X_test_edge, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            X_test_normal = tf.convert_to_tensor(X_test_normal, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            \n",
    "            # MLP_BCE(y_005)\n",
    "            opt = optimization.create_optimizer(init_lr=0.0002,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')\n",
    "            model_sls.compile(loss='BinaryCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "            history = model_sls.fit(X_train, y_sls_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#             plt.plot(history.history['loss'], label='loss')\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "            \n",
    "            # TEST (total)\n",
    "            predicted_total = np.round(model_sls.predict(X_test, verbose=0))\n",
    "            sls_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "            # TEST (edge)\n",
    "            predicted_edge = np.round(model_sls.predict(X_test_edge, verbose=0))\n",
    "            sls_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "            # TEST (normal)\n",
    "            predicted_normal = np.round(model_sls.predict(X_test_normal, verbose=0))\n",
    "            sls_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "                       \n",
    "        res['SLS({})'.format(b)] = [np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)]\n",
    "        print([np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)])          \n",
    "    res.to_csv(\"BERT_RNEWS_5CV(SLS_c0.5)_alphaimproved.csv\", mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9c245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcbced47",
   "metadata": {},
   "source": [
    "# 3. IMDB Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdca36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"IMDB Dataset.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865f1e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['sentiment'] = data['sentiment'].replace(['negative','positive'],[0,1])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9345c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['review'][:150]\n",
    "y_train = data['sentiment'][:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making lists of index\n",
    "idx_0 = []\n",
    "idx_1 = []\n",
    "for i in range(len(y_train)):\n",
    "    if list(y_train)[i] == 0:\n",
    "        idx_0.append(i)\n",
    "    if list(y_train)[i] == 1:\n",
    "        idx_1.append(i)\n",
    "print(len(idx_0), len(idx_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e933ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = idx_0[:50] + idx_1[:50]\n",
    "idx.sort()\n",
    "print(len(idx), idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in idx:\n",
    "    X.append(X_train[i])\n",
    "    y.append(y_train[i])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = y.astype(float)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0446629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "#     net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)     # activation=None\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2334d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "batch = 32\n",
    "epochs = 10\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "steps_per_epoch = int(len(X)*0.4)  # length of train data\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903afa39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hard = pd.DataFrame(y)\n",
    "y_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfe87c",
   "metadata": {},
   "source": [
    "# 3-0. Generating Prob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd38f30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_gen = tf.convert_to_tensor(X, dtype=tf.string, dtype_hint=None, name=None)\n",
    "gen_model = build_classifier_model()\n",
    "opt = optimization.create_optimizer(init_lr=0.0002,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')\n",
    "gen_model.compile(loss='BinaryCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "history = gen_model.fit(X_gen, y_hard, validation_split=0.2, epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "result = gen_model.predict(X_gen, verbose=0)\n",
    "prob_label = list(result.reshape(len(X),))\n",
    "y = pd.DataFrame(prob_label)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa19e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "c = 0.5\n",
    "y_edge = []\n",
    "for i in range(len(y)):\n",
    "    if list(y_hard[0])[i] == 0:\n",
    "        if list(y[0])[i] <= c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "    if list(y_hard[0])[i] == 1:\n",
    "        if list(y[0])[i] >= 1-c:\n",
    "            y_edge.append(0)  # easy sample\n",
    "        else:\n",
    "            y_edge.append(2) # hard sample\n",
    "y_edge = pd.DataFrame(y_edge)\n",
    "y_edge.value_counts()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb67ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "edge_list = list(y_edge[y_edge[0] == 2].index)\n",
    "normal_list = list(y_edge[y_edge[0] == 0].index)\n",
    "print(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7aebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "print(r, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b489666",
   "metadata": {},
   "source": [
    "# 3-1. Focal(Hard) and SLS(Hard/alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f741c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "r = list(y_edge[0]).count(0)/list(y_edge[0]).count(2)   # normal/edge\n",
    "alpha = (r-1)/(2*r)\n",
    "B = [0.00, alpha]\n",
    "\n",
    "for t in range(10):    # 10 times repeat      \n",
    "    res = pd.DataFrame({'Focal':[0, 0, 0]}, index = ['Total','Edge','Normal']) \n",
    "    # Focal\n",
    "    print('#'*50,'Focal','#'*50)\n",
    "    list_total = []\n",
    "    list_edge = []\n",
    "    list_normal = []  \n",
    "    focal_model = build_classifier_model()   \n",
    "\n",
    "    n_iter = 0\n",
    "    for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "        n_iter += 1\n",
    "        X_train = X[train_index]\n",
    "        y_train= y_hard.iloc[train_index]     # train with hard labels\n",
    "        if n_iter == 1:\n",
    "            print(y_train.value_counts())\n",
    "        X_test = X[test_index]\n",
    "        y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "        test_edge_list = []\n",
    "        for index in edge_list:\n",
    "            if index in test_index:\n",
    "                test_edge_list.append(index)\n",
    "        X_test_edge = X[test_edge_list]\n",
    "        y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "        test_normal_list = []\n",
    "        for index in normal_list:\n",
    "            if index in test_index:\n",
    "                test_normal_list.append(index)\n",
    "        X_test_normal = X[test_normal_list]\n",
    "        y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "        \n",
    "        X_train = tf.convert_to_tensor(X_train, dtype=tf.string, dtype_hint=None, name=None)\n",
    "        X_test = tf.convert_to_tensor(X_test, dtype=tf.string, dtype_hint=None, name=None)\n",
    "        X_test_edge = tf.convert_to_tensor(X_test_edge, dtype=tf.string, dtype_hint=None, name=None)\n",
    "        X_test_normal = tf.convert_to_tensor(X_test_normal, dtype=tf.string, dtype_hint=None, name=None)\n",
    "\n",
    "        opt = optimization.create_optimizer(init_lr=0.0002,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')\n",
    "        focal_model.compile(loss='BinaryFocalCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        history = focal_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "        # TEST (total)\n",
    "        predicted_total = np.round(focal_model.predict(X_test, verbose=0))\n",
    "        list_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "        # TEST (edge)\n",
    "        predicted_edge = np.round(focal_model.predict(X_test_edge, verbose=0))\n",
    "        list_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "        # TEST (normal)\n",
    "        predicted_normal = np.round(focal_model.predict(X_test_normal, verbose=0))\n",
    "        list_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "            \n",
    "    res['Focal'] = [np.mean(list_total), np.mean(list_edge), np.mean(list_normal)]\n",
    "    print([np.mean(list_total), np.mean(list_edge), np.mean(list_normal)])\n",
    "\n",
    "    for b in B:\n",
    "        print('#'*50,'SLS',b,'#'*50)\n",
    "        y_sls = []\n",
    "        for i in range(len(y_hard)):\n",
    "            if list(y_hard[0])[i] == 0:\n",
    "                if prob_label[i] <= c:\n",
    "                    y_sls.append(b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(0) # (or 0-b) hard sample\n",
    "            if list(y_hard[0])[i] == 1:\n",
    "                if prob_label[i] >= 1-c:\n",
    "                    y_sls.append(1-b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(1) # (or 1+b) hard sample\n",
    "        y_sls = pd.DataFrame(y_sls)     \n",
    "\n",
    "        sls_total = []\n",
    "        sls_edge = []\n",
    "        sls_normal = []\n",
    "        model_sls = build_classifier_model() \n",
    "\n",
    "        n_iter = 0\n",
    "        for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "            n_iter += 1\n",
    "            X_train = X[train_index]\n",
    "            y_sls_train = y_sls.iloc[train_index]     # train with sls labels\n",
    "            if n_iter == 1:\n",
    "                print(y_sls_train.value_counts())\n",
    "            X_test = X[test_index]\n",
    "            y_test= y_hard.iloc[test_index]     # test with hard labels\n",
    "            test_edge_list = []\n",
    "            for index in edge_list:\n",
    "                if index in test_index:\n",
    "                    test_edge_list.append(index)\n",
    "            X_test_edge = X[test_edge_list]\n",
    "            y_test_edge = y_hard.iloc[test_edge_list]     # test with hard labels\n",
    "            test_normal_list = []\n",
    "            for index in normal_list:\n",
    "                if index in test_index:\n",
    "                    test_normal_list.append(index)\n",
    "            X_test_normal = X[test_normal_list]\n",
    "            y_test_normal = y_hard.iloc[test_normal_list]     # test with hard labels\n",
    "\n",
    "            X_train = tf.convert_to_tensor(X_train, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            X_test = tf.convert_to_tensor(X_test, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            X_test_edge = tf.convert_to_tensor(X_test_edge, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            X_test_normal = tf.convert_to_tensor(X_test_normal, dtype=tf.string, dtype_hint=None, name=None)\n",
    "            \n",
    "            # MLP_BCE(y_005)\n",
    "            opt = optimization.create_optimizer(init_lr=0.0002,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')\n",
    "            model_sls.compile(loss='BinaryCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "            history = model_sls.fit(X_train, y_sls_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch)#, callbacks=[early_stopping])\n",
    "#             plt.plot(history.history['loss'], label='loss')\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "            \n",
    "            # TEST (total)\n",
    "            predicted_total = np.round(model_sls.predict(X_test, verbose=0))\n",
    "            sls_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "            # TEST (edge)\n",
    "            predicted_edge = np.round(model_sls.predict(X_test_edge, verbose=0))\n",
    "            sls_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "            # TEST (normal)\n",
    "            predicted_normal = np.round(model_sls.predict(X_test_normal, verbose=0))\n",
    "            sls_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "                       \n",
    "        res['SLS({})'.format(b)] = [np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)]\n",
    "        print([np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)])          \n",
    "    res.to_csv(\"BERT_IMDB_5CV(SLS_c0.5)_alphaimproved.csv\", mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3f34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf2630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f4043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012cb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244767f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407cc200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ffd3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52329e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
