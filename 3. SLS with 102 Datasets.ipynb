{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f47e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073024c4",
   "metadata": {},
   "source": [
    "## Check if there Null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628d8e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 106):\n",
    "    df = pd.read_csv('ds'+ str(i) +'.csv')\n",
    "    print('='*50)\n",
    "    print('shape of {}:'.format(i), df.shape)\n",
    "    print(df.columns)                            # features names and label\n",
    "    if df.isnull().sum().sum() != 0:             # null?\n",
    "        print(df.isnull().sum())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f3a3d",
   "metadata": {},
   "source": [
    "## Experiment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546db379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.layers import Activation\n",
    "from keras import optimizers\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "hidden_node = 2\n",
    "learning_rate=0.001\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 0)\n",
    "\n",
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_node, input_dim=X.shape[1], kernel_initializer=keras.initializers.he_normal(seed=100)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57779b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f0ab1db",
   "metadata": {},
   "source": [
    "# 1. Generating Probablistic Labels for 102 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08382c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiments for 105-3 Datasets\n",
    "\n",
    "for i in range(1, 106):\n",
    "    if i == 23 or i == 82 or i == 84:\n",
    "        continue\n",
    "    df = pd.read_csv('ds'+ str(i) +'.csv')\n",
    "    print('+'*35, '{}th Dataset'.format(i), '+'*35)\n",
    "    print('<Original Class>\\n', df.iloc[:,-1].value_counts())\n",
    "    \n",
    "    # Make major class as '0' and minor class as '1'\n",
    "    MAJOR = df.iloc[:,-1].value_counts()[df.iloc[:,-1].value_counts() == max(df.iloc[:,-1].value_counts())].index[0]\n",
    "    minor = df.iloc[:,-1].value_counts()[df.iloc[:,-1].value_counts() != max(df.iloc[:,-1].value_counts())].index[0]\n",
    "    df.iloc[:,-1] = df.iloc[:,-1].replace(MAJOR, -100)\n",
    "    df.iloc[:,-1] = df.iloc[:,-1].replace(minor, 1)\n",
    "    df.iloc[:,-1] = df.iloc[:,-1].replace(-100, 0)\n",
    "    print('<Modified Class>\\n', df.iloc[:,-1].value_counts())\n",
    "    print('<Imabalance ratio>\\n', \"{: .2f}:1\".format(df.iloc[:,-1].value_counts()[0]/df.iloc[:,-1].value_counts()[1]))\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    X = (X - X.mean())/X.std()    # Features // Standardization\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    model = mlp_model()\n",
    "    opt = optimizers.Adam(learning_rate = learning_rate)\n",
    "    batch_size = int(X.shape[0] * 0.05)\n",
    "    model.compile(loss='BinaryCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    history = model.fit(X, y, validation_split=0.2, epochs=epochs, verbose=0, batch_size=batch_size)   \n",
    "#     plt.plot(history.history['loss'], label='loss')\n",
    "#     plt.ylim([0, 1])\n",
    "#     plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#     plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#     plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    result = model.predict(X)\n",
    "    prob_label = list(result.reshape(len(X),))\n",
    "    df['prob_label'] = prob_label\n",
    "    df.to_csv(\"Prob_102datasets/ds{}_prob.csv\".format(i), mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12647fe0",
   "metadata": {},
   "source": [
    "# 1-1. Focal(Hard) and SLS(Hard/diverse alphas)_option#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b502a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 106):\n",
    "    if i == 23 or i == 82 or i == 84:\n",
    "        continue\n",
    "    df = pd.read_csv('ds'+str(i)+'_prob'+'.csv', index_col='Unnamed: 0')\n",
    "    y_hard = df.iloc[:, -2]\n",
    "    print('+'*35, '{}th Dataset'.format(i), '+'*35)\n",
    "    print('<Original Class>\\n', y_hard.value_counts())\n",
    "    \n",
    "    # Make major class as '0' and minor class as '1'\n",
    "    MAJOR = y_hard.value_counts()[y_hard.value_counts() == max(y_hard.value_counts())].index[0]\n",
    "    minor = y_hard.value_counts()[y_hard.value_counts() != max(y_hard.value_counts())].index[0]\n",
    "    y_hard = y_hard.replace(MAJOR, -100)\n",
    "    y_hard = y_hard.replace(minor, 1)\n",
    "    y_hard = y_hard.replace(-100, 0)\n",
    "    print('<Modified Class>\\n', y_hard.value_counts())\n",
    "    print('<Imabalance ratio>\\n', \"{: .2f}:1\".format(y_hard.value_counts()[0]/y_hard.value_counts()[1]))\n",
    "    \n",
    "    X = df.iloc[:, :-2]\n",
    "    X = (X - X.mean())/X.std()    # Features // Standardization\n",
    "    y = df.iloc[:, -1]      # prob_label\n",
    "    \n",
    "    res = pd.DataFrame({'Dataset':[0, 0, 0, 0, 0]}, index = ['Acc','Pre','Rec','F1','R-AUC'])\n",
    "    res.iloc[:,0] = [i for b in range(5)]\n",
    "    \n",
    "    # Focal\n",
    "    print('#'*50,'Focal','#'*50)\n",
    "    list_acc = []\n",
    "    list_pre = []\n",
    "    list_rec = []\n",
    "    list_f1 = []\n",
    "    list_rauc = []   \n",
    "    focal_model = mlp_model()\n",
    "\n",
    "    n_iter = 0\n",
    "    for train_index, test_index in skf.split(X, y_hard):  # straticiation by y_hard(binary label)\n",
    "        n_iter += 1\n",
    "        X_train = X.iloc[train_index]\n",
    "        y_train= y_hard.iloc[train_index]\n",
    "        if n_iter == 1:\n",
    "            print(y_train.value_counts())\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_test= y_hard.iloc[test_index]\n",
    "#         print('#'*10,'{0}th CV'.format(n_iter),'#'*10)\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        y_train = y_train.astype(float)\n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "        y_test = y_test.astype(float)\n",
    "\n",
    "        focal_model.compile(loss='BinaryFocalCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.001), metrics=['accuracy'])\n",
    "        history = focal_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch_size)#, callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        predicted = np.round(focal_model.predict(X_test, verbose=0))\n",
    "        list_acc.append(metrics.accuracy_score(y_test, predicted))\n",
    "        list_pre.append(metrics.precision_score(y_test, predicted))\n",
    "        list_rec.append(metrics.recall_score(y_test, predicted))\n",
    "        list_f1.append(metrics.f1_score(y_test, predicted))\n",
    "        list_rauc.append(metrics.roc_auc_score(y_test, predicted))\n",
    "    res['Focal'] = [np.mean(list_acc), np.mean(list_pre), np.mean(list_rec), np.mean(list_f1), np.mean(list_rauc)]\n",
    "    print([np.mean(list_acc), np.mean(list_pre), np.mean(list_rec), np.mean(list_f1), np.mean(list_rauc)])\n",
    "    \n",
    "    B = [0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10]     # SLS with LS/NLS\n",
    "    c = 0.3  # criterion decides easy/hard\n",
    "    for b in B:\n",
    "        print('#'*50,'SLS',b,'#'*50)\n",
    "        y_005 = []\n",
    "        for i in range(len(y_hard)):\n",
    "            if y_hard[i] == 0:\n",
    "                if y[i] <= c:\n",
    "                    y_005.append(b)  # easy sample\n",
    "                else:\n",
    "                    y_005.append(0) # (or 0-b) hard sample\n",
    "            if y_hard[i] == 1:\n",
    "                if y[i] >= 1-c:\n",
    "                    y_005.append(1-b)  # easy sample\n",
    "                else:\n",
    "                    y_005.append(1) # (or 1+b) hard sample\n",
    "        y_005 = pd.DataFrame(y_005)      \n",
    "\n",
    "        bce005_acc = []\n",
    "        bce005_pre = []\n",
    "        bce005_rec = []\n",
    "        bce005_f1 = []\n",
    "        bce005_rocauc = []\n",
    "        model_005 = mlp_model()\n",
    "#             early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        n_iter = 0\n",
    "        for train_index, test_index in skf.split(X, y_hard):  # straticiation by y_hard(binary label)\n",
    "            n_iter += 1\n",
    "            X_train = X.iloc[train_index]\n",
    "            y_005_train= y_005.iloc[train_index]\n",
    "            if n_iter == 1:\n",
    "                print(y_005_train.value_counts())\n",
    "            X_test = X.iloc[test_index]\n",
    "            y_test= y_hard.iloc[test_index]  # test with real(actual) label y\n",
    "#                 print('#'*10,'{0}th CV'.format(n_iter),'#'*10)\n",
    "            X_train = np.array(X_train)\n",
    "            y_005_train = np.array(y_005_train)\n",
    "            y_005_train = y_005_train.astype(float)\n",
    "            X_test = np.array(X_test)\n",
    "            y_test = np.array(y_test)\n",
    "            y_test = y_test.astype(float)\n",
    "\n",
    "            # MLP_BCE(y_005)\n",
    "            model_005.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.001), metrics=['accuracy'])\n",
    "            history = model_005.fit(X_train, y_005_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch_size)#, callbacks=[early_stopping])\n",
    "#             plt.plot(history.history['loss'], label='loss')\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "            predicted = np.round(model_005.predict(X_test, verbose=0))\n",
    "            bce005_acc.append(metrics.accuracy_score(y_test, predicted))\n",
    "            bce005_pre.append(metrics.precision_score(y_test, predicted))\n",
    "            bce005_rec.append(metrics.recall_score(y_test, predicted))\n",
    "            bce005_f1.append(metrics.f1_score(y_test, predicted))\n",
    "            bce005_rocauc.append(metrics.roc_auc_score(y_test, predicted))\n",
    "        res['SLS({})'.format(b)] = [np.mean(bce005_acc), np.mean(bce005_pre), np.mean(bce005_rec), np.mean(bce005_f1), np.mean(bce005_rocauc)]\n",
    "        print([np.mean(bce005_acc), np.mean(bce005_pre), np.mean(bce005_rec), np.mean(bce005_f1), np.mean(bce005_rocauc)])\n",
    "    res.to_csv(\"102datasets_5CV(SLS_opt#1_c0.3).csv\", mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb75438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c475ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e923b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
