{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f47e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073024c4",
   "metadata": {},
   "source": [
    "## Check if there Null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 106):\n",
    "    df = pd.read_csv('ds'+ str(i) +'.csv')\n",
    "    print('='*50)\n",
    "    print('shape of {}:'.format(i), df.shape)\n",
    "    print(df.columns)                            # features names and label\n",
    "    if df.isnull().sum().sum() != 0:             # null?\n",
    "        print(df.isnull().sum())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f3a3d",
   "metadata": {},
   "source": [
    "## Experiment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546db379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.layers import Activation\n",
    "from keras import optimizers\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "hidden_node = 2\n",
    "learning_rate=0.001\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 0)\n",
    "\n",
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_node, input_dim=X.shape[1], kernel_initializer=keras.initializers.he_normal(seed=100)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57779b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f0ab1db",
   "metadata": {},
   "source": [
    "# 1. Generating Probablistic Labels for 102 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08382c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiments for 105-3 Datasets\n",
    "\n",
    "for i in range(1, 106):\n",
    "    if i == 23 or i == 82 or i == 84:\n",
    "        continue\n",
    "    df = pd.read_csv(r'ds'+ str(i) +'.csv')\n",
    "    print('+'*35, '{}th Dataset'.format(i), '+'*35)\n",
    "    print('<Original Class>\\n', df.iloc[:,-1].value_counts())\n",
    "    \n",
    "    # Make major class as '0' and minor class as '1'\n",
    "    MAJOR = df.iloc[:,-1].value_counts()[df.iloc[:,-1].value_counts() == max(df.iloc[:,-1].value_counts())].index[0]\n",
    "    minor = df.iloc[:,-1].value_counts()[df.iloc[:,-1].value_counts() != max(df.iloc[:,-1].value_counts())].index[0]\n",
    "    df.iloc[:,-1] = df.iloc[:,-1].replace(MAJOR, -100)\n",
    "    df.iloc[:,-1] = df.iloc[:,-1].replace(minor, 1)\n",
    "    df.iloc[:,-1] = df.iloc[:,-1].replace(-100, 0)\n",
    "    y_hard = list(df.iloc[:,-1])\n",
    "    print('<Modified Class>\\n', df.iloc[:,-1].value_counts())\n",
    "    print('<Imabalance ratio>\\n', \"{: .2f}:1\".format(df.iloc[:,-1].value_counts()[0]/df.iloc[:,-1].value_counts()[1]))\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    X = (X - X.mean())/X.std()    # Features // Standardization\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    model = mlp_model()\n",
    "    opt = optimizers.Adam(learning_rate = learning_rate)\n",
    "    batch_size = 32\n",
    "    model.compile(loss='BinaryCrossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    history = model.fit(X, y, validation_split=0.2, epochs=epochs, verbose=0, batch_size=batch_size)   \n",
    "#     plt.plot(history.history['loss'], label='loss')\n",
    "#     plt.ylim([0, 1])\n",
    "#     plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#     plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#     plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    result = model.predict(X)\n",
    "    df.rename(columns={df.columns[-1]:'y'}, inplace=True)\n",
    "    prob_label = list(result.reshape(len(X),))\n",
    "    df['y'] = prob_label\n",
    "    df['y_hard'] = y_hard\n",
    "    df.to_csv(\"Prob_102datasets/ds{}_prob.csv\".format(i), mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12647fe0",
   "metadata": {},
   "source": [
    "# 1-1. Focal(Hard) and SLS(Hard/alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b502a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = 0.3  # criterion decides easy/hard\n",
    "for i in range(1, 106):\n",
    "    if i == 23 or i == 82 or i == 84:\n",
    "        continue\n",
    "    df_data = pd.read_csv(r'ds'+str(i)+'_prob'+'.csv', index_col='Unnamed: 0')\n",
    "    print('+'*35, '{}th Dataset'.format(i), '+'*35)\n",
    "    y = df_data.iloc[:, -2]\n",
    "    y_hard = df_data.iloc[:, -1]\n",
    "    y_edge = []\n",
    "    for j in range(len(y)):\n",
    "        if y_hard[j] == 0:\n",
    "            if y[j] <= c:\n",
    "                y_edge.append(0)  # easy sample\n",
    "            else:\n",
    "                y_edge.append(2) # hard sample\n",
    "        if y_hard[j] == 1:\n",
    "            if y[j] >= 1-c:\n",
    "                y_edge.append(0)  # easy sample\n",
    "            else:\n",
    "                y_edge.append(2) # hard sample\n",
    "    df_data['y_edge'] = y_edge\n",
    "    df_data['y_sls'] = [0 for i in range(len(y))]\n",
    "    if y_edge.count(0) == 0 or y_edge.count(2) == 0:  # if normal or edge = 0, all samples are edges or normal, meaning nothing\n",
    "        continue\n",
    "    if y_edge.count(0) <= 4 or y_edge.count(2) <= 4:  # too small # of samples cannot be used both for train and test\n",
    "        continue\n",
    "    else:\n",
    "        r = y_edge.count(0)/y_edge.count(2)  # normal/edge\n",
    "        if r <= 2:\n",
    "            r = 2  # although edge cases are more than normal cases, we want to give more weights to edge cases\n",
    "        alpha = (r-1)/(2*r)\n",
    "#     print(\"normal:\", y_edge.count(0), \"edge:\", y_edge.count(2))\n",
    "    print(\"r:\", r, \"alpha:\", alpha)\n",
    "    \n",
    "    X = df_data.iloc[:, :-4]\n",
    "    X = (X - X.mean())/X.std()    # Features // Standardization\n",
    "    \n",
    "    res = pd.DataFrame({'Dataset':[0, 0, 0]}, index = ['Total','Edge','Normal'])\n",
    "    res.iloc[:,0] = [i for b in range(3)]\n",
    "    \n",
    "    # Focal\n",
    "    print('#'*50,'Focal','#'*50)\n",
    "    list_total = []\n",
    "    list_edge = []\n",
    "    list_normal = []   \n",
    "    focal_model = mlp_model()\n",
    "\n",
    "    n_iter = 0\n",
    "    for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "        n_iter += 1\n",
    "        df_train = df_data.iloc[train_index]\n",
    "        X_train = df_train.iloc[:,:-4]\n",
    "        y_train = df_train.iloc[:,-3]  # train with hard labels\n",
    "        if n_iter == 1:\n",
    "            print(y_train.value_counts())        \n",
    "        df_test = df_data.iloc[test_index]\n",
    "        X_test = df_test.iloc[:,:-4]\n",
    "        y_test = df_test.iloc[:,-3]  # test with hard labels\n",
    "        df_test_edge = df_test[df_test['y_edge'] == 2.00]\n",
    "#         print(len(df_test_edge))\n",
    "        X_test_edge = df_test_edge.iloc[:,:-4]\n",
    "        y_test_edge = df_test_edge.iloc[:,-3]  # test with hard labels\n",
    "        df_test_normal = df_test[df_test['y_edge'] == 0.00]\n",
    "#         print(len(df_test_normal))\n",
    "        X_test_normal = df_test_normal.iloc[:,:-4]\n",
    "        y_test_normal = df_test_normal.iloc[:,-3]  # test with hard labels\n",
    "        \n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        y_train = y_train.astype(float)    \n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "        y_test = y_test.astype(float)\n",
    "        X_test_edge = np.array(X_test_edge)\n",
    "        y_test_edge = np.array(y_test_edge)\n",
    "        y_test_edge = y_test_edge.astype(float)\n",
    "        X_test_normal = np.array(X_test_normal)\n",
    "        y_test_normal = np.array(y_test_normal)\n",
    "        y_test_normal = y_test_normal.astype(float)\n",
    "\n",
    "        focal_model.compile(loss='BinaryFocalCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.001), metrics=['accuracy'])\n",
    "        history = focal_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch_size)#, callbacks=[early_stopping])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "        # TEST (total)\n",
    "        predicted_total = np.round(focal_model.predict(X_test, verbose=0))\n",
    "        list_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "        # TEST (edge)\n",
    "        predicted_edge = np.round(focal_model.predict(X_test_edge, verbose=0))\n",
    "        list_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "        # TEST (normal)\n",
    "        predicted_normal = np.round(focal_model.predict(X_test_normal, verbose=0))\n",
    "        list_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "            \n",
    "    res['Focal'] = [np.mean(list_total), np.mean(list_edge), np.mean(list_normal)]\n",
    "    print([np.mean(list_total), np.mean(list_edge), np.mean(list_normal)])\n",
    "    \n",
    "    B = [0.00, alpha]     # Hard/SLS with alpha\n",
    "    for b in B:\n",
    "        print('#'*50,'SLS',b,'#'*50)\n",
    "        y_sls = []\n",
    "        for i in range(len(y_hard)):\n",
    "            if y_hard[i] == 0:\n",
    "                if y[i] <= c:\n",
    "                    y_sls.append(b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(0) # hard sample\n",
    "            if y_hard[i] == 1:\n",
    "                if y[i] >= 1-c:\n",
    "                    y_sls.append(1-b)  # easy sample\n",
    "                else:\n",
    "                    y_sls.append(1) # hard sample\n",
    "        df_data['y_sls'] = y_sls  # update the column (y_sls)            \n",
    "\n",
    "        sls_total = []\n",
    "        sls_edge = []\n",
    "        sls_normal = []\n",
    "        model_sls = mlp_model()\n",
    "        \n",
    "        n_iter = 0\n",
    "        for train_index, test_index in skf.split(X, y_edge):  # straticiation by y_edge\n",
    "            n_iter += 1\n",
    "            df_train = df_data.iloc[train_index]\n",
    "            X_train = df_train.iloc[:,:-4]\n",
    "            y_sls_train = df_train.iloc[:,-1]  # train with sls labels\n",
    "            if n_iter == 1:\n",
    "                print(y_sls_train.value_counts())                \n",
    "            df_test = df_data.iloc[test_index]\n",
    "            X_test = df_test.iloc[:,:-4]\n",
    "            y_test = df_test.iloc[:,-3]  # test with hard labels\n",
    "            df_test_edge = df_test[df_test['y_edge'] == 2.00]\n",
    "            X_test_edge = df_test_edge.iloc[:,:-4]\n",
    "            y_test_edge = df_test_edge.iloc[:,-3]  # test with hard labels\n",
    "            df_test_normal = df_test[df_test['y_edge'] == 0.00]\n",
    "            X_test_normal = df_test_normal.iloc[:,:-4]\n",
    "            y_test_normal = df_test_normal.iloc[:,-3]  # test with hard labels\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            y_sls_train = np.array(y_sls_train)\n",
    "            y_sls_train = y_sls_train.astype(float)\n",
    "            X_test = np.array(X_test)\n",
    "            y_test = np.array(y_test)\n",
    "            y_test = y_test.astype(float)\n",
    "            X_test_edge = np.array(X_test_edge)\n",
    "            y_test_edge = np.array(y_test_edge)\n",
    "            y_test_edge = y_test_edge.astype(float)\n",
    "            X_test_normal = np.array(X_test_normal)\n",
    "            y_test_normal = np.array(y_test_normal)\n",
    "            y_test_normal = y_test_normal.astype(float)\n",
    "\n",
    "            # MLP_BCE(y_005)\n",
    "            model_sls.compile(loss='BinaryCrossentropy', optimizer=optimizers.Adam(learning_rate = 0.001), metrics=['accuracy'])\n",
    "            history = model_sls.fit(X_train, y_sls_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch_size)#, callbacks=[early_stopping])\n",
    "#             plt.plot(history.history['loss'], label='loss')\n",
    "#             plt.ylim([0, 1])\n",
    "#             plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#             plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "\n",
    "            # TEST (total)\n",
    "            predicted_total = np.round(model_sls.predict(X_test, verbose=0))\n",
    "            sls_total.append(metrics.accuracy_score(y_test, predicted_total))\n",
    "            # TEST (edge)\n",
    "            predicted_edge = np.round(model_sls.predict(X_test_edge, verbose=0))\n",
    "            sls_edge.append(metrics.accuracy_score(y_test_edge, predicted_edge))\n",
    "            # TEST (normal)\n",
    "            predicted_normal = np.round(model_sls.predict(X_test_normal, verbose=0))\n",
    "            sls_normal.append(metrics.accuracy_score(y_test_normal, predicted_normal))\n",
    "        \n",
    "        if b == 0.00:\n",
    "            res['SLS(0.0)'] = [np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)]\n",
    "        else:\n",
    "            res['SLS(alpha)'] = [np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)]\n",
    "            res['alpha'] = [b,b,b]\n",
    "        print([np.mean(sls_total), np.mean(sls_edge), np.mean(sls_normal)])\n",
    "    res.to_csv(\"102datasets_5CV(SLS_c0.3)_alphaimproved.csv\", mode = 'a', float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e923b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
